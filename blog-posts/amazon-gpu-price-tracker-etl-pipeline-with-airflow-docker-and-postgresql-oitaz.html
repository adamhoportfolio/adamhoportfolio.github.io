<!DOCTYPE html><html data-wf-page="6675fef784ac8a70b3b907bf" data-wf-site="6675fef784ac8a70b3b90776" data-wf-status="1" lang="en" data-wf-locale="en" data-wf-collection="6675fef784ac8a70b3b907ea" data-wf-item-slug="amazon-gpu-price-tracker-etl-pipeline-with-airflow-docker-and-postgresql-oitaz"><head><meta charset="utf-8"/><title>Blog Posts</title><meta content="width=device-width, initial-scale=1" name="viewport"/><meta content="Webflow" name="generator"/><link href="../css/webflow-style.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com" rel="preconnect"/><link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous"/><script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script><script type="text/javascript">WebFont.load({  google: {    families: ["Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic","Roboto:300,regular,500","Unbounded:regular"]  }});</script><script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script><link href="../images/favicon.png" rel="shortcut icon" type="image/x-icon"/><link href="../images/app-icon.png" rel="apple-touch-icon"/></head><body><div class="header-section nav-menu"><div class="w-container"><a href="/" class="blog-home-link w-inline-block"></a><div class="navigation-bar"><a href="/" class="nav-link">Projects</a><a href="/about" class="nav-link">about</a><a href="https://github.com/wacsvn" target="_blank" class="nav-link">Github</a></div></div><a href="/about" class="w-inline-block"><h1 class="blog-name"><strong class="bold-text navigation-logo">ADAM HO</strong></h1></a><div class="text-block">DATAÂ SCIENCE, B.S.</div></div><div class="section"><div data-w-id="5d559596-fa66-860b-d991-7b6b5a6154eb" style="opacity:0" class="w-container"><img alt="" src="../images/amazongputhumb.jpg" sizes="80vw" srcset="../images/amazongputhumb-p-500.jpg 500w, ../images/amazongputhumb-p-800.jpg 800w, ../images/amazongputhumb-p-1080.jpg 1080w, ../images/amazongputhumb.jpg 1337w" class="main-image"/><h1 data-w-expand="category" class="post-heading">Amazon GPU Price Tracker - ETL Pipeline w/ airflow, postgres</h1><div class="byline-wrapper"><div class="byline-text">February 23, 2025</div><div class="byline-text">In</div><a href="/categories/data-engineering" class="byline-link">Data Engineering</a></div><div class="blog-content w-richtext"><h2>Introduction</h2><p>In today&#x27;s rapidly changing tech market, keeping track of GPU prices can be valuable for both enthusiasts and professionals. Graphics cards have seen significant price fluctuations over the past few years due to crypto mining booms, chip shortages, and new product releases. To monitor these trends efficiently, I built an automated data pipeline that extracts GPU listings from Amazon, transforms the data, and loads it into a PostgreSQL database for analysis.</p><p>This blog post walks through the development of this ETL (Extract, Transform, Load) pipeline using Apache Airflow for orchestration, Docker for containerization, and PostgreSQL for data storage.</p><p>â€</p><h2>ğŸ“Š Data Pipeline Architecture</h2><p>Amazon Website â†’ Web Scraper â†’ Data Cleaning â†’ PostgreSQL Database</p><p> Â  Â  Â â†‘ Â  Â  Â  Â  Â  Â  Â  Â  â†‘ Â  Â  Â  Â  Â  Â  Â â†‘ Â  Â  Â  Â  Â  Â  Â  â†‘</p><p> Â  Â  Â â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Airflow DAG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</p><p>â€</p><h2>Project Goals</h2><p>The primary objectives for this project were to:</p><ol role="list"><li>Automatically collect GPU pricing data from Amazon on a daily basis</li><li>Clean and transform the raw data into a structured format</li><li>Store the processed data in a relational database</li><li>Create a reusable and maintainable pipeline using modern data engineering tools</li><li>Deploy the solution using containers for portability and ease of setup</li></ol><p>â€</p><h2>Tech Stack Overview</h2><p>For this project, I selected the following technologies:</p><ul role="list"><li><strong>Apache Airflow</strong>: For workflow orchestration and scheduling</li><li><strong>Docker &amp; Docker Compose</strong>: For containerization and simplified deployment</li><li><strong>PostgreSQL</strong>: For structured data storage</li><li><strong>BeautifulSoup</strong>: For web scraping</li><li><strong>Pandas</strong>: For data manipulation and transformation</li><li><strong>Python</strong>: As the core programming language</li></ul><p>â€</p><h2>Setting Up the Development Environment</h2><p>The project uses Docker Compose to set up a complete development environment with all necessary services. The Docker Compose configuration sets up several containers:</p><ul role="list"><li>Airflow webserver and scheduler</li><li>PostgreSQL database</li><li>Redis for Airflow&#x27;s task queue</li></ul><p>To start the environment, simply run:</p><blockquote><code>docker-compose up -d</code></blockquote><p>â€</p><h2>Building the Data Pipeline with Airflow DAGs</h2><figure class="w-richtext-align-center w-richtext-figure-type-image"><div><img src="../images/latest_only_with_trigger.png" alt="DAGs â€” Airflow Documentation" loading="lazy"/></div><figcaption>Example of an Airflow DAG with chained workflow triggers</figcaption></figure><p>â€</p><p>Apache Airflow uses Directed Acyclic Graphs (DAGs) to define workflows. Here&#x27;s an overview of the DAG I created for this project:</p><h3>Key Components of the DAG</h3><p>The DAG consists of three main tasks:</p><ol role="list"><li>Fetch GPU data from Amazon</li><li>Create a database table if it doesn&#x27;t exist</li><li>Insert the processed data into PostgreSQL</li></ol><p>Let&#x27;s look at some code snippets from the implementation:</p><p>â€</p><h3>1. Extracting Data with BeautifulSoup</h3><p>The extraction function uses BeautifulSoup to parse Amazon search results and extract relevant information:</p><p><code>def get_amazon_data_gpus(num_gpus, ti):<br/> Â  ...<br/><br/> Â  Â while len(gpus) &lt; num_gpus:<br/> Â  Â  Â  Â url = f&quot;{base_url}&amp;page={page}&quot;<br/> Â  Â  Â  Â <br/> Â  Â  Â  Â # Send a request to the URL<br/> Â  Â  Â  Â response = requests.get(url, headers=headers)<br/> Â  Â  Â  Â <br/> Â  Â  Â  Â # Check if the request was successful<br/> Â  Â  Â  Â if response.status_code == 200:<br/> Â  Â  Â  Â  Â  Â # Parse the content of the request with BeautifulSoup<br/> Â  Â  Â  Â  Â  Â soup = BeautifulSoup(response.content, &quot;html.parser&quot;)<br/> Â  Â  Â  Â  Â  Â <br/> Â  Â  Â  Â  Â  Â # Find GPU containers (you may need to adjust the class names based on the actual HTML structure)<br/> Â  Â  Â  Â  Â  Â gpu_containers = soup.find_all(&quot;div&quot;, {&quot;class&quot;: &quot;s-result-item&quot;})<br/> Â  Â  Â  Â  Â  Â <br/> Â  Â  Â  Â  Â  Â # Loop through the GPU containers and extract data<br/> Â  Â  Â  Â  Â  Â for gpu in gpu_containers:<br/> Â  Â  Â  Â  Â  Â  Â  Â title = gpu.find(&quot;span&quot;, {&quot;class&quot;: &quot;a-text-normal&quot;})<br/> Â  Â  Â  Â  Â  Â  Â  Â brand = gpu.find(&quot;a&quot;, {&quot;class&quot;: &quot;a-size-base&quot;})<br/> Â  Â  Â  Â  Â  Â  Â  Â price = gpu.find(&quot;span&quot;, {&quot;class&quot;: &quot;a-price-whole&quot;})<br/> Â  Â  Â  Â  Â  Â  Â  Â rating = gpu.find(&quot;span&quot;, {&quot;class&quot;: &quot;a-icon-alt&quot;})<br/> Â  Â  Â  Â  Â  Â  Â  Â <br/> Â  Â  Â  Â  Â  Â  Â  Â if title and brand and price and rating:<br/> Â  Â  Â  Â  Â  Â  Â  Â  Â  Â gpu_title = title.text.strip()<br/> Â  Â  Â  Â  Â  Â  Â  Â  Â  Â <br/> Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Check if title has been seen before<br/> Â  Â  Â  Â  Â  Â  Â  Â  Â  Â if gpu_title not in seen_titles:<br/> Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â seen_titles.add(gpu_title)<br/> Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â gpus.append({<br/> Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â &quot;Title&quot;: gpu_title,<br/> Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â &quot;Brand&quot;: brand.text.strip(),<br/> Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â &quot;Price&quot;: price.text.strip(),<br/> Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â &quot;Rating&quot;: rating.text.strip(),<br/> Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â })</code></p><figure class="w-richtext-align-center w-richtext-figure-type-image"><div><img src="../images/amazon-overall-pick-gpu-gaming-pc-featured.jpeg" alt="Amazon&#x27;s Overall Picks for gaming PCs" loading="lazy"/></div><figcaption>The scraper will parse through all listings that result from querying for &quot;graphics cards&quot;, extracting specific fields such as brand, price, and rating</figcaption></figure><p>â€</p><p>This function:</p><ul role="list"><li>Sends requests to Amazon&#x27;s search pages for GPUs</li><li>Parses the HTML content</li><li>Extracts GPU title, brand, price, and rating information</li><li>Handles pagination to collect the specified number of items</li><li>Removes duplicates based on product titles</li><li>Stores the results in a Pandas DataFrame for further processing</li></ul><p>â€</p><h3>2. Creating the Database Table</h3><p>The database schema is defined using a PostgresOperator:</p><p><code>create_table_task = PostgresOperator(<br/> Â  Â task_id=&#x27;create_table&#x27;,<br/> Â  Â postgres_conn_id=&#x27;books_connection&#x27;,<br/> Â  Â sql=&quot;&quot;&quot;<br/> Â  Â CREATE TABLE IF NOT EXISTS gpus (<br/> Â  Â  Â  Â id SERIAL PRIMARY KEY,<br/> Â  Â  Â  Â title TEXT NOT NULL,<br/> Â  Â  Â  Â brand TEXT,<br/> Â  Â  Â  Â price TEXT,<br/> Â  Â  Â  Â rating TEXT<br/> Â  Â );<br/> Â  Â &quot;&quot;&quot;,<br/> Â  Â dag=dag,<br/>)<br/></code></p><p>This task creates a table with columns for the GPU&#x27;s title, brand, price, and rating if it doesn&#x27;t already exist.</p><p>â€</p><h3>3. Loading Data into PostgreSQL</h3><p>Finally, the data is loaded into PostgreSQL using a PostgresHook:</p><p><code>def insert_gpu_data_into_postgres(ti):<br/> Â  Â gpu_data = ti.xcom_pull(key=&#x27;gpu_data&#x27;, task_ids=&#x27;fetch_gpu_data&#x27;)<br/> Â  Â if not gpu_data:<br/> Â  Â  Â  Â raise ValueError(&quot;No GPU data found&quot;)<br/><br/> Â  Â postgres_hook = PostgresHook(postgres_conn_id=&#x27;books_connection&#x27;)<br/> Â  Â insert_query = &quot;&quot;&quot;<br/> Â  Â INSERT INTO gpus (title, brand, price, rating)<br/> Â  Â VALUES (%s, %s, %s, %s)<br/> Â  Â &quot;&quot;&quot;<br/> Â  Â for gpu in gpu_data:<br/> Â  Â  Â  Â postgres_hook.run(insert_query, parameters=(gpu[&#x27;Title&#x27;], gpu[&#x27;Brand&#x27;], gpu[&#x27;Price&#x27;], gpu[&#x27;Rating&#x27;]))</code></p><p><code>â€</code></p><figure class="w-richtext-align-center w-richtext-figure-type-image"><div><img src="../images/screenshot.png" alt="pgAdmin - PostgreSQL Tools" loading="lazy"/></div><figcaption>pgAdmin also provides the ability to monitor transaction rates and server activity</figcaption></figure><p>This function:</p><ul role="list"><li>Retrieves the processed data from Airflow&#x27;s XCom (cross-communication) system</li><li>Establishes a connection to PostgreSQL</li><li>Inserts each GPU record into the database</li></ul><p>â€</p><h3>Defining Task Dependencies</h3><p>The workflow&#x27;s execution order is defined by setting dependencies between tasks:</p><blockquote><code>fetch_gpu_data_task &gt;&gt; create_table_task &gt;&gt; insert_gpu_data_task</code></blockquote><p>This ensures that tasks run in the correct sequence: fetch data â†’ create table â†’ insert data.</p><p>â€</p><h2>Web Scraping Challenges and Solutions</h2><p>Web scraping Amazon presented several challenges:</p><p>â€</p><h3>Challenge 1: Bot Detection</h3><p>Amazon implements measures to detect and block scraping activities. To mitigate this:</p><ul role="list"><li>I used realistic User-Agent headers:</li></ul><p><code>headers = {<br/> Â  Â &quot;Referer&quot;: &#x27;https://www.amazon.com/&#x27;,<br/> Â  Â &quot;Sec-Ch-Ua&quot;: &quot;Not_A Brand&quot;,<br/> Â  Â &quot;Sec-Ch-Ua-Mobile&quot;: &quot;?0&quot;,<br/> Â  Â &quot;Sec-Ch-Ua-Platform&quot;: &quot;macOS&quot;,<br/> Â  Â &#x27;User-agent&#x27;: &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36&#x27;<br/>}<br/></code></p><ul role="list"><li>Implemented implicit request throttling through pagination</li><li>Added error handling for failed requests</li></ul><p>â€</p><h3>Challenge 2: HTML Structure Changes</h3><p>Amazon&#x27;s page structure can change, breaking scrapers. My solution:</p><ul role="list"><li>Used more general CSS selectors where possible</li><li>Implemented robust error handling</li><li>Added checks to ensure all required data elements are present before processing</li></ul><p>â€</p><h2>Future Enhancements</h2><p>While the current pipeline serves its purpose well, there are several potential improvements:</p><ol role="list"><li><strong>Price History Tracking</strong>: Modify the schema to track price changes over time</li><li><strong>Alert System</strong>: Set up alerts for price drops on specific models</li><li><strong>Data Visualization Dashboard</strong>: Create a dashboard for easier data exploration</li><li><strong>Expanded Product Information</strong>: Collect additional specs like VRAM, clock speeds, etc.</li><li><strong>Sentiment Analysis</strong>: Analyze product reviews to gauge customer satisfaction</li></ol><p>â€</p><h2>Conclusion</h2><p>Building this ETL pipeline provided valuable insights into working with Airflow, web scraping, and data engineering practices. The containerized setup makes it easy to deploy and maintain, while the scheduled nature of Airflow ensures fresh data is consistently available.</p><p>This project demonstrates how modern data tools can be combined to create practical solutions for real-world problems like price tracking and market analysis. The same approach could be adapted for other product categories or e-commerce sites with minimal modifications.</p><p>â€</p><h2>Resources and References</h2><ul role="list"><li><a href="https://github.com/wacsvn/amazon-gpus-airflow">GitHub Repository</a></li><li><a href="https://airflow.apache.org/docs/">Apache Airflow Documentation</a></li><li><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup Documentation</a></li><li><a href="https://docs.docker.com/">Docker Documentation</a></li></ul></div></div></div><div class="footer"><div class="w-container"><div><a href="https://github.com/wacsvn" target="_blank" class="social-icon-link w-inline-block"><img src="../images/github-white.svg" width="30" alt="" class="image"/></a><a href="#" class="social-icon-link w-inline-block"></a><a href="https://www.linkedin.com/in/adam-ho-056852208/" target="_blank" class="social-icon-link w-inline-block"><img src="../images/linkedin-white.svg" width="30" alt=""/></a></div><div class="footer-text">2025</div></div></div><script src="../js/jquery.js" type="text/javascript"  ></script><script src="../js/webflow.schunk.4a394eb5af8156f2.js" type="text/javascript"></script><script src="../js/webflow.schunk.3bf3c06809a02500.js" type="text/javascript"></script><script src="../js/webflow-script.js" type="text/javascript"></script></body></html>